{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMKMxe9fgx/qv4oWM7ePN1s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ridazaneb/IndoFashionCLIP/blob/main/finalsubsetfinetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqUYNv5uEu-8"
      },
      "outputs": [],
      "source": [
        "# subsetfinetune_final.py\n",
        "# ---------------------------------------------\n",
        "# End-to-end Colab pipeline for fine-tuning CLIP on a South Asian\n",
        "# fashion dataset.\n",
        "# ---------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Install all required libraries (run once per session)\n",
        "!pip install -q torch torchvision transformers accelerate pandas matplotlib scikit-learn\n"
      ],
      "metadata": {
        "id": "goirWHObE5IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Imports and random seed setup for reproducibility\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    CLIPProcessor,\n",
        "    CLIPVisionModel,\n",
        "    get_cosine_schedule_with_warmup\n",
        ")\n",
        "from torch.amp import autocast\n",
        "from torch.cuda.amp import GradScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from google.colab import drive\n",
        "\n",
        "# Setting a global seed to ensure results can be reproduced\n",
        "SEED = 45\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "nOI_XUamFEDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Mount Google Drive to access data and save models\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Iz3F5zgFKqg",
        "outputId": "92ed81bb-6c14-4eb4-8e77-acced7289758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Define file paths and hyperparameters (one place to edit)\n",
        "BASE_DIR         = '/content/drive/MyDrive/IndoFashion'\n",
        "DATA_DIR         = os.path.join(BASE_DIR, 'data')    # JSON metadata folder\n",
        "IMAGES_ROOT      = os.path.join(BASE_DIR, 'images')  # train/val/test image folders\n",
        "MODELS_DIR       = os.path.join(BASE_DIR, 'models')  # where to save models and mappings\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)               # ensure models directory exists\n"
      ],
      "metadata": {
        "id": "4bMzRkuIFPEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "SUBSET_PER_CLASS = 1000     # max images per class for the training subset\n",
        "EPOCHS           = 12       # total training epochs\n",
        "BATCH_SIZE       = 32       # number of images per batch\n",
        "LR               = 3e-5     # learning rate for optimizer\n",
        "PATIENCE         = 3        # early stopping patience\n",
        "WARMUP_RATIO     = 0.1      # fraction of steps used for LR warmup\n",
        "DEVICE           = 'cuda' if torch.cuda.is_available() else 'cpu'  # CPU or GPU"
      ],
      "metadata": {
        "id": "2AJVcS4kFSBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Function to load newline-delimited JSON metadata into a DataFrame\n",
        "#    Each JSON line must contain either 'image_path' or 'image_url',\n",
        "#    and 'class_label' or 'label'.\n",
        "def load_jsonl_split(path: str, split: str) -> pd.DataFrame:\n",
        "    records = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            obj = json.loads(line)\n",
        "            # Determine the image file basename\n",
        "            if 'image_path' in obj:\n",
        "                basename = os.path.basename(obj['image_path'])\n",
        "            elif 'image_url' in obj:\n",
        "                basename = os.path.basename(obj['image_url'])\n",
        "            else:\n",
        "                raise ValueError(\"JSON needs 'image_path' or 'image_url'.\")\n",
        "            # Determine the class label field\n",
        "            label = obj.get('class_label') or obj.get('label')\n",
        "            if label is None:\n",
        "                raise ValueError(\"JSON needs 'class_label' or 'label'.\")\n",
        "            # Store the relative path and label\n",
        "            records.append({\n",
        "                'image_path': os.path.join(split, basename),\n",
        "                'class_label': label\n",
        "            })\n",
        "    # Convert list of dicts into a DataFrame\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# Load metadata for train, val, test splits\n",
        "train_df = load_jsonl_split(os.path.join(DATA_DIR, 'train_data.json'), 'train')\n",
        "val_df   = load_jsonl_split(os.path.join(DATA_DIR,   'val_data.json'),   'val')\n",
        "test_df  = load_jsonl_split(os.path.join(DATA_DIR,  'test_data.json'),  'test')\n"
      ],
      "metadata": {
        "id": "--XJOOtoFXbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Create a balanced training subset: up to SUBSET_PER_CLASS images per label\n",
        "subset_list = []\n",
        "for label, group in train_df.groupby('class_label'):\n",
        "    # Sample min(total, SUBSET_PER_CLASS) images from each class\n",
        "    sampled = group.sample(n=min(len(group), SUBSET_PER_CLASS), random_state=SEED)\n",
        "    subset_list.append(sampled)\n",
        "small_train_df = pd.concat(subset_list).reset_index(drop=True)\n",
        "print(f\"Training subset: {len(small_train_df)} images across {small_train_df['class_label'].nunique()} classes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnZ15hJLFd6v",
        "outputId": "4f319ff8-3a4c-43ff-d88f-c2f23051ef02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training subset: 15000 images across 15 classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Create mapping from class label string to integer ID and back\n",
        "labels   = sorted(small_train_df['class_label'].unique())\n",
        "label2id = {lbl: idx for idx, lbl in enumerate(labels)}\n",
        "id2label = {idx: lbl for lbl, idx in label2id.items()}\n",
        "# Map labels to integer IDs in the DataFrames\n",
        "small_train_df['label_id'] = small_train_df['class_label'].map(label2id)\n",
        "val_df['label_id']         = val_df['class_label'].map(label2id)\n",
        "test_df['label_id']        = test_df['class_label'].map(label2id)\n"
      ],
      "metadata": {
        "id": "FC_G0-o7FlbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Set up data augmentation for training and consistent resizing for validation/test\n",
        "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),     # random crop + resize for robustness\n",
        "    transforms.RandomHorizontalFlip(p=0.5),# random flip to augment left/right poses\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.1) # random brightness/contrast/saturation/hue\n",
        "])\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),                # resize to 256x256\n",
        "    transforms.CenterCrop(224)             # central 224x224 crop to match CLIP input\n",
        "])"
      ],
      "metadata": {
        "id": "kMRSMCFDFtKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30cdcba-570b-42b9-f642-7830d212bbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Custom Dataset class to load images and return CLIP-ready tensors\n",
        "class IndoFashionSubset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, root_dir: str, transforms=None):\n",
        "        self.df         = df.reset_index(drop=True)  # keep a copy\n",
        "        self.root       = root_dir                   # root images folder\n",
        "        self.transforms = transforms                 # optional image augmentations\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Fetch row and build full image path\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.root, row['image_path'])\n",
        "        # Load with PIL and apply transforms\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "        # Use CLIPProcessor to handle normalization & tensor conversion\n",
        "        pv    = processor(images=img, return_tensors='pt').pixel_values[0]\n",
        "        label = int(row['label_id'])\n",
        "        return pv, label"
      ],
      "metadata": {
        "id": "q8QNhBPAFwdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) Prepare DataLoaders for train, val, test\n",
        "train_loader = DataLoader(\n",
        "    IndoFashionSubset(small_train_df, IMAGES_ROOT, train_transforms),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    IndoFashionSubset(val_df, IMAGES_ROOT, val_transforms),\n",
        "    batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    IndoFashionSubset(test_df, IMAGES_ROOT, val_transforms),\n",
        "    batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "u2TaR7q7F1ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11) Define the fine-tuning model: CLIP vision backbone + small linear head\n",
        "vision_model = CLIPVisionModel.from_pretrained('openai/clip-vit-base-patch32')\n",
        "class ClipFineTune(nn.Module):\n",
        "    def __init__(self, backbone: CLIPVisionModel, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.backbone   = backbone                                # pre-trained CLIP\n",
        "        self.classifier = nn.Linear(backbone.config.hidden_size, num_classes)\n",
        "    def forward(self, pixel_values):\n",
        "        # Extract pooled embeddings from the vision transformer\n",
        "        pooled = self.backbone(pixel_values=pixel_values).pooler_output\n",
        "        # Classify into your fashion categories\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "model = ClipFineTune(vision_model, len(labels)).to(DEVICE)\n",
        "# Freeze all but the last 4 encoder layers + head to adapt higher-level features\n",
        "for name, param in model.backbone.named_parameters():\n",
        "    if not any(layer in name for layer in ['encoder.layer.8', 'encoder.layer.9', 'encoder.layer.10', 'encoder.layer.11']):\n",
        "        param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "r5xuywuwF4E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12) Set up optimizer (only trainable params), LR scheduler, loss function, and scaler\n",
        "optimizer = optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad], lr=LR\n",
        ")\n",
        "total_steps  = EPOCHS * len(train_loader)                     # total training iterations\n",
        "warmup_steps = int(WARMUP_RATIO * total_steps)            # warm-up schedule # Calculating warmup_steps here\n",
        "scheduler    = get_cosine_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()  # cross-entropy for multi-class classification\n",
        "scaler    = GradScaler()           # mixed-precision gradient scaler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ugU_xeVF7ce",
        "outputId": "562efd31-f153-4b81-a893-a5eae017357b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8920d2671f29>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler    = GradScaler()           # mixed-precision gradient scaler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13) Function to train one epoch (returns average loss & accuracy)\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    running_loss, running_correct, running_total = 0, 0, 0\n",
        "    for pixels, labels in train_loader:\n",
        "        pixels, labels = pixels.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        # Mixed-precision forward & backward\n",
        "        with autocast(device_type='cuda', enabled=(DEVICE=='cuda')):\n",
        "            logits = model(pixels)\n",
        "            loss   = criterion(logits, labels)\n",
        "        scaler.scale(loss).backward()  # scale gradients\n",
        "        scaler.step(optimizer)        # update weights\n",
        "        scaler.update()               # update scale for next step\n",
        "        scheduler.step()              # update LR per iteration\n",
        "        # Track metrics\n",
        "        running_loss   += loss.item() * labels.size(0)\n",
        "        preds           = logits.argmax(dim=-1)\n",
        "        running_correct += (preds == labels).sum().item()\n",
        "        running_total   += labels.size(0)\n",
        "    avg_loss = running_loss / running_total\n",
        "    avg_acc  = running_correct / running_total\n",
        "    return avg_loss, avg_acc\n"
      ],
      "metadata": {
        "id": "HqvD69GWGHtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14) Function to evaluate on val or test (also returns detailed reports)\n",
        "def eval_epoch(loader, split_name='Val'):\n",
        "    model.eval()\n",
        "    eval_loss, eval_correct, eval_total = 0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for pixels, labels in loader:\n",
        "            pixels, labels = pixels.to(DEVICE), labels.to(DEVICE)\n",
        "            logits         = model(pixels)\n",
        "            loss           = criterion(logits, labels)\n",
        "            eval_loss     += loss.item() * labels.size(0)\n",
        "            preds          = logits.argmax(dim=-1)\n",
        "            eval_correct  += (preds == labels).sum().item()\n",
        "            eval_total    += labels.size(0)\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "    avg_loss = eval_loss / eval_total\n",
        "    avg_acc  = eval_correct / eval_total\n",
        "    print(f\"{split_name} Loss: {avg_loss:.4f} | {split_name} Acc: {avg_acc:.4f}\")\n",
        "    # Create per-class metrics and confusion matrix\n",
        "    # Get unique labels from all_labels\n",
        "    unique_labels = sorted(list(set(all_labels)))\n",
        "    # Map unique labels to original label names\n",
        "    target_names = [id2label[label_id] for label_id in unique_labels]\n",
        "\n",
        "    # Filter all_preds and all_labels to only include classes in target_names\n",
        "    filtered_preds = []\n",
        "    filtered_labels = []\n",
        "    for pred, label in zip(all_preds, all_labels):\n",
        "        if label in unique_labels:\n",
        "            filtered_preds.append(pred)\n",
        "            filtered_labels.append(label)\n",
        "\n",
        "    report = classification_report(filtered_labels, filtered_preds, target_names=target_names, zero_division=0)  # Include zero_division=0 or 1 to handle cases with 0 in denominator\n",
        "    cm     = confusion_matrix(filtered_labels, filtered_preds)\n",
        "\n",
        "    return avg_loss, avg_acc, report, cm"
      ],
      "metadata": {
        "id": "dS0UvyTCGLra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15) Main training loop with early stopping and metric logging\n",
        "train_losses, val_losses = [], []\n",
        "train_accs,    val_accs  = [], []\n",
        "best_val_acc, patience_counter = 0.0, PATIENCE\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
        "    tr_loss, tr_acc = train_epoch()\n",
        "    vl_loss, vl_acc, vl_report, vl_cm = eval_epoch(val_loader, 'Val')\n",
        "    # Save metrics for plotting\n",
        "    train_losses.append(tr_loss); val_losses.append(vl_loss)\n",
        "    train_accs.append(tr_acc);    val_accs.append(vl_acc)\n",
        "    # Show three classes with lowest F1 to monitor weaknesses\n",
        "    # Convert classification report string to dictionary\n",
        "    import re\n",
        "    report_dict = {}\n",
        "    for line in vl_report.split('\\n'):\n",
        "        if re.match(r'^\\s*[a-zA-Z_]+\\s+\\d', line):  # Match lines starting with class names followed by numbers\n",
        "            parts = re.split(r'\\s+', line.strip())\n",
        "            class_name = parts[0]\n",
        "            # Check if parts has enough elements to avoid IndexError\n",
        "            if len(parts) >= 5:\n",
        "                try:\n",
        "                    report_dict[class_name] = {\n",
        "                        'precision': float(parts[1]),\n",
        "                        'recall': float(parts[2]),\n",
        "                        'f1-score': float(parts[3]),\n",
        "                        'support': int(parts[4])\n",
        "                    }\n",
        "                except ValueError:\n",
        "                    # Handle potential ValueError if conversion fails\n",
        "                    print(f\"Warning: Could not parse line: {line.strip()}\")\n",
        "                    continue  # Skip this line and move to the next\n",
        "            else:\n",
        "                #print(f\"Warning: Skipping line with insufficient data: {line.strip()}\")\n",
        "            #report_dict[class_name] = {\n",
        "            #    'precision': float(parts[1]),\n",
        "            #    'recall': float(parts[2]),\n",
        "            #    'f1-score': float(parts[3]),\n",
        "            #    'support': int(parts[4])\n",
        "            #}\n",
        "\n",
        "    f1_scores = {cls: report_dict.get(cls, {}).get('f1-score', 0.0) for cls in labels} # Get f1-score, default to 0.0 if not found\n",
        "    lowest    = sorted(f1_scores.items(), key=lambda x: x[1])[:3]\n",
        "    print(\"Lowest F1: \", lowest)\n",
        "    # Early stopping check\n",
        "    if vl_acc > best_val_acc:\n",
        "        best_val_acc = vl_acc\n",
        "        torch.save(model.state_dict(), os.path.join(MODELS_DIR, 'best.pt'))\n",
        "        patience_counter = PATIENCE\n",
        "    else:\n",
        "        patience_counter -= 1\n",
        "        if patience_counter <= 0:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ycOBJuTPaDK",
        "outputId": "bb118f21-44c0-41e4-b4ef-4281778686b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Time taken for subset=1000 with 12 epochs: 24m"
      ],
      "metadata": {
        "id": "EL20CDbdeopf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16) Plot training & validation loss/accuracy curves for visual grading\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses,   label='Val Loss')\n",
        "plt.title('Loss per Epoch'); plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(train_accs, label='Train Acc')\n",
        "plt.plot(val_accs,   label='Val Acc')\n",
        "plt.title('Accuracy per Epoch'); plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1kdx-BxXGzC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17) Final evaluation on the test set using the best saved model\n",
        "print(\"\\nTesting with best saved model:\")\n",
        "model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'best.pt')))\n",
        "_, test_acc, test_report, test_cm = eval_epoch(test_loader, 'Test')\n",
        "print(\"Test Classification Report:\\n\", test_report)\n"
      ],
      "metadata": {
        "id": "0nydst8GG17P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time taken for subset=1000 with 12 epochs: 8m"
      ],
      "metadata": {
        "id": "BqU6lo-wf_m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18) Save the class ID ↔ label mapping for deployment\n",
        "with open(os.path.join(MODELS_DIR, 'id2label.json'), 'w') as f:\n",
        "    json.dump(id2label, f)\n",
        "\n",
        "print(f\"\\nDone. Best Validation Accuracy: {best_val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "V0Ay0iSZG5sW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}